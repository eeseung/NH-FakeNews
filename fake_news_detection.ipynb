{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_news_detection.inpyb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7LzDBaHQtlB"
      },
      "source": [
        "### Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2x1GHfGVZ1y"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v06tgzo3_-RJ"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC4p2ftxkqx8"
      },
      "source": [
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUL-d5P5nBLD"
      },
      "source": [
        "cd Mecab-ko-for-Google-Colab/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieWJwsonki5u"
      },
      "source": [
        "! bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsQYRxLVVQpv"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np      \r\n",
        "import pandas as pd       \r\n",
        "import re\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tqdm.auto import tqdm\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import konlpy\r\n",
        "from konlpy.tag import Mecab\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvTv7F65VeKD"
      },
      "source": [
        "# 데이터 불러오기\r\n",
        "train = pd.read_csv(\"/content/gdrive/MyDrive/NH-FakeNews/news_train.csv\")\r\n",
        "test = pd.read_csv(\"/content/gdrive/MyDrive/NH-FakeNews/news_test.csv\")\r\n",
        "train_Y = train['info']\r\n",
        "\r\n",
        "# 데이터 확인\r\n",
        "print('train count:', len(train))\r\n",
        "print('test count:', len(test))\r\n",
        "\r\n",
        "# pd.set_option('display.max_columns', 1000)\r\n",
        "# print(train[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFV7y5uzfb95"
      },
      "source": [
        "### Prepocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUuNOuatBUYL"
      },
      "source": [
        "def preprocess_text(text_list):\r\n",
        "    # 불용어 사전\r\n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', '의', '하', '에']\r\n",
        "    # 형태소 분석기\r\n",
        "    tokenizer = Mecab()\r\n",
        "    token_list = []\r\n",
        "    \r\n",
        "    for text in tqdm(text_list):\r\n",
        "        # 한글과 영어 소문자 제외하고 모두 제거\r\n",
        "        txt = re.sub('[^가-힣a-z]', ' ', text.lower())\r\n",
        "        # 형태소 분석\r\n",
        "        token = tokenizer.morphs(txt)\r\n",
        "        # 불용어 제외한 결과 추출\r\n",
        "        token = [t for t in token if t not in stopwords or type(t) != float]\r\n",
        "        token_list.append(token)\r\n",
        "        \r\n",
        "    return token_list, tokenizer\r\n",
        "\r\n",
        "train['token'], mecab = preprocess_text(train['content'])\r\n",
        "train['token']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnzPIdXG9KCd"
      },
      "source": [
        "sentence_len = [len(sentence) for sentence in train['token']]\r\n",
        "sentences = train['token']\r\n",
        "plt.hist(sentence_len, bins=88)\r\n",
        "plt.xlabel('length of contents')\r\n",
        "plt.ylabel('number of contents')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print('content 최대 길이 :',max(sentence_len))\r\n",
        "print('content 평균 길이 :',sum(sentence_len)/len(sentences))\r\n",
        "print(sum([int(l<=50) for l in sentence_len]))\r\n",
        "print(sum([int(l<=50) for l in sentence_len])/len(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3I-IW9M9hW-"
      },
      "source": [
        "# 단어 정제 및 문장 길이 축소\r\n",
        "sentences = train['token']\r\n",
        "sentences_new = []\r\n",
        "for sentence in sentences:\r\n",
        "    sentences_new.append([word[:20] for word in sentence][:50])\r\n",
        "sentences = sentences_new\r\n",
        "\r\n",
        "# 데이터 확인\r\n",
        "# for i in range(5):\r\n",
        "#     print(sentences[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRD8ilrRfo6F"
      },
      "source": [
        "### Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H14l_wF7-gsj"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=40000)\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "\r\n",
        "train_X = tokenizer.texts_to_sequences(sentences)\r\n",
        "train_X = pad_sequences(train_X)\r\n",
        "\r\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxPoMGeRgxjw"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCO0b8Pm-sN7"
      },
      "source": [
        "# 모델 정의\r\n",
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, 128, input_length=50), \r\n",
        "    tf.keras.layers.Dropout(0.1),\r\n",
        "    tf.keras.layers.LSTM(units=128),\r\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "\r\n",
        "# optimizer 정의\r\n",
        "adam = tf.keras.optimizers.Adam(lr = 0.0005)\r\n",
        "\r\n",
        "# 모델 compile\r\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44EQV0um-7TW"
      },
      "source": [
        "# 과적합 방지를 위한 EarlyStopping\r\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_accuracy')\r\n",
        "\r\n",
        "# 모델 학습\r\n",
        "history = model.fit(train_X, train_Y, epochs=10, batch_size=512, validation_split=0.25, callbacks=[earlystop_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vua19maA_CAm"
      },
      "source": [
        "# 모델 학습 결과 시각화\r\n",
        "plt.figure(figsize=(12, 4))\r\n",
        "\r\n",
        "plt.subplot(1, 2, 1)\r\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\r\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "plt.plot(history.history['accuracy'], 'g-', label='accuracy')\r\n",
        "plt.plot(history.history['val_accuracy'], 'k--', label='val_accuracy')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylim(0.7, 1)\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TeXgcpANWeg"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHnC8y94Xh9T"
      },
      "source": [
        "# test 데이터 전처리 + 형태소 분석\r\n",
        "test['token'], mecab = preprocess_text(test['content'])\r\n",
        "\r\n",
        "# 단어 정제 및 문장 길이 축소\r\n",
        "sentences_test = test['token']\r\n",
        "sentences_new_test = []\r\n",
        "for sentence_test in sentences_test:\r\n",
        "    sentences_new_test.append([word[:20] for word in sentence_test][:50])\r\n",
        "sentences_test = sentences_new_test\r\n",
        "\r\n",
        "# 데이터 확인\r\n",
        "# for i in range(5):\r\n",
        "#     print(sentences[i])\r\n",
        "\r\n",
        "# Vectorization\r\n",
        "test_X = tokenizer.texts_to_sequences(sentences_test)\r\n",
        "test_X = pad_sequences(test_X)\r\n",
        "\r\n",
        "# 예측\r\n",
        "test_Y = model.predict(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvd72M2dEfxw"
      },
      "source": [
        "# 결과 제출\r\n",
        "submission = pd.read_csv(\"/content/gdrive/MyDrive/NH-FakeNews/fake_news_submission.csv\")\r\n",
        "submission.loc[:,'info'] = np.where(test_Y> 0.5, 1,0).reshape(-1)\r\n",
        "submission.loc[:,[\"id\",\"info\"]].to_csv(\"/content/gdrive/MyDrive/NH-FakeNews/fake_news_submission.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}